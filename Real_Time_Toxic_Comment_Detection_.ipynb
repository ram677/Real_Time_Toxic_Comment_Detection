{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Project**\n",
        "\n",
        "Problem Statement : **Real-Time Toxic Comment Detection**\n",
        "\n",
        "- Goal: Detect toxic or offensive comments in social media posts.\n",
        "\n",
        "- Tools: sklearn, nltk, or DistilBERT with small batch size\n",
        "\n",
        "- Tasks:\n",
        "\n",
        "    - Use the Jigsaw Toxic Comment dataset (or a smaller sample)\n",
        "\n",
        "    - Train logistic regression or use a small transformer\n",
        "  \n",
        "    - Build a simple web interface or browser extension to scan text and classify\n",
        "\n",
        "- Bonus: Highlight toxic keywords using color in output.\n"
      ],
      "metadata": {
        "id": "fyt_8kT4Pwwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9NFRz0m9BWVB",
        "outputId": "dcf12079-5672-4916-d9b0-b851798c2877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMulti-Label Toxic Comment Detection - Logistic Regression + Gradio (from Google Drive)\\n\\nThis notebook demonstrates building a multi-label toxic comment classifier\\npredicting probabilities for 6 types of toxicity.\\nIt uses Logistic Regression with OneVsRestClassifier, TF-IDF, loads data\\nfrom Google Drive, and deploys with a Gradio web interface.\\nIncludes bonus keyword highlighting.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Multi-Label Toxic Comment Detection - Logistic Regression + Gradio (from Google Drive)\n",
        "\n",
        "This notebook demonstrates building a multi-label toxic comment classifier\n",
        "predicting probabilities for 6 types of toxicity.\n",
        "It uses Logistic Regression with OneVsRestClassifier, TF-IDF, loads data\n",
        "from Google Drive, and deploys with a Gradio web interface.\n",
        "Includes bonus keyword highlighting.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 1. Setup: Install Libraries and Import Modules\n",
        "!pip install numpy pandas scikit-learn nltk joblib gradio --quiet\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score, classification_report, hamming_loss, jaccard_score, accuracy_score as subset_accuracy\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "\n",
        "# Download necessary NLTK data (if not already present)\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet.zip') # Check for the zip file, more robust\n",
        "except LookupError: # Catch LookupError directly\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords.zip')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Import NLTK submodules after ensuring resources are available\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "print(\"Libraries installed and imported.\")\n",
        "print(\"NLTK resources checked/downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHItUhijCHRl",
        "outputId": "b7374e8a-a3ba-4964-f356-bc707ad0fb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibraries installed and imported.\n",
            "NLTK resources checked/downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Mount Google Drive and Specify Dataset Path\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# --- DATASET PATH ---\n",
        "BASE_DRIVE_PATH = '/content/drive/MyDrive/Jigsaw_Toxic_Comment_dataset'\n",
        "DRIVE_DATASET_PATH_TRAIN = os.path.join(BASE_DRIVE_PATH, 'train.csv')\n",
        "DRIVE_DATASET_PATH_TEST = os.path.join(BASE_DRIVE_PATH, 'test.csv')\n",
        "DRIVE_DATASET_PATH_TEST_LABELS = os.path.join(BASE_DRIVE_PATH, 'test_labels.csv')\n",
        "\n",
        "\n",
        "# Define the toxicity labels we are interested in\n",
        "TOXIC_LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "if not os.path.exists(DRIVE_DATASET_PATH_TRAIN):\n",
        "    print(f\"ERROR: Training data file not found at {DRIVE_DATASET_PATH_TRAIN}\")\n",
        "    print(\"Please ensure the file 'train.csv' exists in the specified Google Drive folder.\")\n",
        "    print(f\"Expected folder: {BASE_DRIVE_PATH}\")\n",
        "else:\n",
        "    print(f\"Training data path set to: {DRIVE_DATASET_PATH_TRAIN}\")\n",
        "    print(f\"Test data path set to: {DRIVE_DATASET_PATH_TEST}\")\n",
        "    print(f\"Test labels path set to: {DRIVE_DATASET_PATH_TEST_LABELS}\")\n",
        "    print(f\"Target labels: {TOXIC_LABELS}\")\n",
        "\n",
        "# You can quickly check if the files exist:\n",
        "print(f\"\\nChecking file existence:\")\n",
        "print(f\"Train CSV exists: {os.path.exists(DRIVE_DATASET_PATH_TRAIN)}\")\n",
        "print(f\"Test CSV exists: {os.path.exists(DRIVE_DATASET_PATH_TEST)}\") # Will be False if test.csv is not there\n",
        "print(f\"Test Labels CSV exists: {os.path.exists(DRIVE_DATASET_PATH_TEST_LABELS)}\") # Will be False if test_labels.csv is not there"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tUfpqlfCQ2E",
        "outputId": "5ccc3492-6b6e-49dd-96d9-e7c04d2fe873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "Training data path set to: /content/drive/MyDrive/Jigsaw_Toxic_Comment_dataset/train.csv\n",
            "Test data path set to: /content/drive/MyDrive/Jigsaw_Toxic_Comment_dataset/test.csv\n",
            "Test labels path set to: /content/drive/MyDrive/Jigsaw_Toxic_Comment_dataset/test_labels.csv\n",
            "Target labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
            "\n",
            "Checking file existence:\n",
            "Train CSV exists: True\n",
            "Test CSV exists: True\n",
            "Test Labels CSV exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Load and Sample Data from Google Drive\n",
        "# --- Configuration ---\n",
        "SAMPLE_SIZE = 30000 # Adjust as needed. Set to None to use full dataset (might be slow/memory intensive).\n",
        "DATA_FILE_TRAIN = DRIVE_DATASET_PATH_TRAIN\n",
        "\n",
        "df_processed = pd.DataFrame()\n",
        "\n",
        "if not os.path.exists(DATA_FILE_TRAIN):\n",
        "     print(f\"ERROR: {DATA_FILE_TRAIN} not found. Please check the path in Cell 2.\")\n",
        "else:\n",
        "    print(f\"Loading training data from {DATA_FILE_TRAIN}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(DATA_FILE_TRAIN)\n",
        "        print(\"Original training data shape:\", df.shape)\n",
        "\n",
        "        # Handle potential missing values in comments BEFORE sampling\n",
        "        df['comment_text'].fillna(\"missing\", inplace=True)\n",
        "\n",
        "        if SAMPLE_SIZE and SAMPLE_SIZE < len(df):\n",
        "            print(f\"Sampling {SAMPLE_SIZE} records...\")\n",
        "            df_processed = df.sample(n=SAMPLE_SIZE, random_state=42).copy()\n",
        "            print(\"Sampled data shape:\", df_processed.shape)\n",
        "        else:\n",
        "            df_processed = df.copy()\n",
        "            print(\"Using full dataset. Shape:\", df_processed.shape)\n",
        "\n",
        "        print(\"\\nData Sample (first 5 rows of processed data):\")\n",
        "        print(df_processed.head())\n",
        "        print(\"\\nLabel distribution in processed data (sum of labels):\")\n",
        "        print(df_processed[TOXIC_LABELS].sum())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing data: {e}\")\n",
        "\n",
        "if df_processed.empty:\n",
        "    print(\"\\n---! DATAFRAME IS EMPTY !--- Halting execution. Check file path and content.\")\n",
        "    # exit() # Uncomment to forcibly stop if dataframe is empty\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nK3iXuWDU0v",
        "outputId": "25b154fc-5dc6-4a97-c8c2-10a69a3af8a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data from /content/drive/MyDrive/Jigsaw_Toxic_Comment_dataset/train.csv...\n",
            "Original training data shape: (159571, 8)\n",
            "Sampling 30000 records...\n",
            "Sampled data shape: (30000, 8)\n",
            "\n",
            "Data Sample (first 5 rows of processed data):\n",
            "                      id                                       comment_text  \\\n",
            "119105  7ca72b5b9c688e9e  Geez, are you forgetful!  We've already discus...   \n",
            "131631  c03f72fd8f8bf54f  Carioca RFA \\n\\nThanks for your support on my ...   \n",
            "125326  9e5b8e8fc1ff2e84  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...   \n",
            "111256  5332799e706665a6  Pseudoscience category? \\n\\nI'm assuming that ...   \n",
            "83590   dfa7d8f0b4366680  (and if such phrase exists, it would be provid...   \n",
            "\n",
            "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
            "119105      0             0        0       0       0              0  \n",
            "131631      0             0        0       0       0              0  \n",
            "125326      0             0        0       0       0              0  \n",
            "111256      0             0        0       0       0              0  \n",
            "83590       0             0        0       0       0              0  \n",
            "\n",
            "Label distribution in processed data (sum of labels):\n",
            "toxic            2846\n",
            "severe_toxic      290\n",
            "obscene          1592\n",
            "threat             69\n",
            "insult           1502\n",
            "identity_hate     272\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-0dc0adfadc8e>:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['comment_text'].fillna(\"missing\", inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Text Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words_set = set(stopwords.words('english')) # Use a consistent variable name\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words_set and word.isalpha()]\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "if not df_processed.empty:\n",
        "    print(\"Preprocessing comments...\")\n",
        "    df_processed['cleaned_comment'] = df_processed['comment_text'].apply(preprocess_text)\n",
        "    print(\"Preprocessing complete.\")\n",
        "    print(\"\\nSample Original vs Cleaned:\")\n",
        "    print(df_processed[['comment_text', 'cleaned_comment']].head())\n",
        "else:\n",
        "    print(\"Skipping preprocessing as DataFrame is empty.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNyQWgWlEVHg",
        "outputId": "8ea28e3a-5d20-47d7-bf5f-56a5ab3d792b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing comments...\n",
            "Preprocessing complete.\n",
            "\n",
            "Sample Original vs Cleaned:\n",
            "                                             comment_text  \\\n",
            "119105  Geez, are you forgetful!  We've already discus...   \n",
            "131631  Carioca RFA \\n\\nThanks for your support on my ...   \n",
            "125326  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...   \n",
            "111256  Pseudoscience category? \\n\\nI'm assuming that ...   \n",
            "83590   (and if such phrase exists, it would be provid...   \n",
            "\n",
            "                                          cleaned_comment  \n",
            "119105  geez forgetful weve already discussed marx ana...  \n",
            "131631  carioca rfa thanks support request adminship f...  \n",
            "125326                   birthday worry enjoy ur daytalke  \n",
            "111256  pseudoscience category im assuming article pse...  \n",
            "83590   phrase exists would provided search engine eve...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Feature Extraction (TF-IDF) and Data Splitting\n",
        "if not df_processed.empty:\n",
        "    X_text = df_processed['cleaned_comment']\n",
        "    y_labels = df_processed[TOXIC_LABELS].values\n",
        "\n",
        "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "        X_text, y_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training text samples: {len(X_train_text)}\")\n",
        "    print(f\"Test text samples: {len(X_test_text)}\")\n",
        "    print(f\"Shape of y_train: {y_train.shape}\")\n",
        "    print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer(max_features=15000, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
        "\n",
        "    print(\"Fitting TF-IDF vectorizer and transforming text data...\")\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
        "    X_test_tfidf = vectorizer.transform(X_test_text)\n",
        "    print(\"TF-IDF transformation complete.\")\n",
        "    print(\"Shape of TF-IDF matrix (Train):\", X_train_tfidf.shape)\n",
        "    print(\"Shape of TF-IDF matrix (Test):\", X_test_tfidf.shape)\n",
        "else:\n",
        "    print(\"Skipping TF-IDF and splitting as DataFrame is empty.\")\n",
        "    X_train_tfidf, X_test_tfidf, y_train, y_test = None, None, None, None\n",
        "    vectorizer = None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0ZeK0bEa-i",
        "outputId": "e626c441-5460-4908-b5ed-546631356e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training text samples: 24000\n",
            "Test text samples: 6000\n",
            "Shape of y_train: (24000, 6)\n",
            "Shape of y_test: (6000, 6)\n",
            "Fitting TF-IDF vectorizer and transforming text data...\n",
            "TF-IDF transformation complete.\n",
            "Shape of TF-IDF matrix (Train): (24000, 15000)\n",
            "Shape of TF-IDF matrix (Test): (6000, 15000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Model Training (OneVsRestClassifier with Logistic Regression)\n",
        "if X_train_tfidf is not None and y_train is not None:\n",
        "    base_lr = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced', C=1.0)\n",
        "    model = OneVsRestClassifier(base_lr)\n",
        "\n",
        "    print(\"Training Multi-Label model (OneVsRestClassifier with Logistic Regression)...\")\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "else:\n",
        "    print(\"Skipping model training as data is not available.\")\n",
        "    model = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4n2U1PdElhe",
        "outputId": "7e985a35-6ffa-4ad1-b314-8af5c4235521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Multi-Label model (OneVsRestClassifier with Logistic Regression)...\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Model Evaluation\n",
        "if model and X_test_tfidf is not None and y_test is not None:\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred_proba = model.predict_proba(X_test_tfidf)\n",
        "    y_pred_binary = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(\"\\n--- Multi-Label Metrics ---\")\n",
        "    h_loss = hamming_loss(y_test, y_pred_binary)\n",
        "    print(f\"Hamming Loss: {h_loss:.4f}\")\n",
        "    subset_acc = subset_accuracy(y_test, y_pred_binary)\n",
        "    print(f\"Subset Accuracy (Exact Match Ratio): {subset_acc:.4f}\")\n",
        "    j_score_sample = jaccard_score(y_test, y_pred_binary, average='samples')\n",
        "    print(f\"Jaccard Score (Sample-wise Average): {j_score_sample:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Per-Label Evaluation ---\")\n",
        "    print(\"ROC AUC Scores (per label):\")\n",
        "    for i, label in enumerate(TOXIC_LABELS):\n",
        "        if len(np.unique(y_test[:, i])) > 1:\n",
        "            auc = roc_auc_score(y_test[:, i], y_pred_proba[:, i])\n",
        "            print(f\"  {label}: {auc:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {label}: Not enough classes in y_test for ROC AUC (single class present).\")\n",
        "\n",
        "    print(\"\\nClassification Report (per label, based on binary predictions):\")\n",
        "    report = classification_report(y_test, y_pred_binary, target_names=TOXIC_LABELS, zero_division=0)\n",
        "    print(report)\n",
        "else:\n",
        "    print(\"Skipping model evaluation as model or test data is not available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_1MzdfZEpAI",
        "outputId": "8850423c-3598-4b11-9361-be58d49d76bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model...\n",
            "\n",
            "--- Multi-Label Metrics ---\n",
            "Hamming Loss: 0.0296\n",
            "Subset Accuracy (Exact Match Ratio): 0.8840\n",
            "Jaccard Score (Sample-wise Average): 0.0487\n",
            "\n",
            "--- Per-Label Evaluation ---\n",
            "ROC AUC Scores (per label):\n",
            "  toxic: 0.9613\n",
            "  severe_toxic: 0.9688\n",
            "  obscene: 0.9737\n",
            "  threat: 0.9730\n",
            "  insult: 0.9626\n",
            "  identity_hate: 0.9424\n",
            "\n",
            "Classification Report (per label, based on binary predictions):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.64      0.77      0.70       543\n",
            " severe_toxic       0.25      0.72      0.37        53\n",
            "      obscene       0.69      0.78      0.73       297\n",
            "       threat       0.35      0.47      0.40        15\n",
            "       insult       0.53      0.74      0.62       286\n",
            "identity_hate       0.18      0.48      0.26        48\n",
            "\n",
            "    micro avg       0.55      0.75      0.64      1242\n",
            "    macro avg       0.44      0.66      0.51      1242\n",
            " weighted avg       0.59      0.75      0.66      1242\n",
            "  samples avg       0.05      0.07      0.06      1242\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Save Model and Vectorizer\n",
        "MODEL_FILENAME = 'multilabel_toxic_model.joblib'\n",
        "VECTORIZER_FILENAME = 'multilabel_tfidf_vectorizer.joblib'\n",
        "\n",
        "if model and vectorizer:\n",
        "    print(f\"Saving model to {MODEL_FILENAME}...\")\n",
        "    joblib.dump(model, MODEL_FILENAME)\n",
        "    print(f\"Saving vectorizer to {VECTORIZER_FILENAME}...\")\n",
        "    joblib.dump(vectorizer, VECTORIZER_FILENAME)\n",
        "    print(\"Model and vectorizer saved to Colab's temporary storage.\")\n",
        "    # To save to Google Drive:\n",
        "    # drive_model_path = os.path.join(BASE_DRIVE_PATH, MODEL_FILENAME)\n",
        "    # drive_vectorizer_path = os.path.join(BASE_DRIVE_PATH, VECTORIZER_FILENAME)\n",
        "    # joblib.dump(model, drive_model_path)\n",
        "    # joblib.dump(vectorizer, drive_vectorizer_path)\n",
        "    # print(f\"Model saved to Google Drive: {drive_model_path}\")\n",
        "    # print(f\"Vectorizer saved to Google Drive: {drive_vectorizer_path}\")\n",
        "else:\n",
        "    print(\"Skipping saving model/vectorizer as they were not trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTNOYf-Et6z",
        "outputId": "cac29172-2942-4a6a-ccc4-190c45396d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to multilabel_toxic_model.joblib...\n",
            "Saving vectorizer to multilabel_tfidf_vectorizer.joblib...\n",
            "Model and vectorizer saved to Colab's temporary storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Define Prediction Function for Gradio and Keyword Highlighting\n",
        "\n",
        "GENERIC_TOXIC_KEYWORDS = [\n",
        "    'idiot', 'stupid', 'dumb', 'hate', 'kill', 'murder', 'die', 'nazi', 'racist',\n",
        "    'fuck', 'shit', 'bitch', 'asshole', 'cunt', 'moron', 'retard', 'ugly', 'loser',\n",
        "    'gay', 'jew', 'faggot', 'suck', 'pussy', 'whore', 'slut', 'terrorist', 'pig',\n",
        "    'scum', 'cock', 'dick', 'fat', 'freak', 'libtard', 'maggot', 'rape', 'retarded',\n",
        "    # --- EXPAND THIS LIST SIGNIFICANTLY ---\n",
        "    'fuk', 'fck', 'b!tch', 'a$$hole', 'kike', 'n1gger', 'chink', 'dyke', 'tranny'\n",
        "]\n",
        "# Lemmatize keywords for better matching with preprocessed input\n",
        "GENERIC_TOXIC_KEYWORDS_SET = set([lemmatizer.lemmatize(word.lower()) for word in GENERIC_TOXIC_KEYWORDS])\n",
        "\n",
        "loaded_model = None\n",
        "loaded_vectorizer = None\n",
        "\n",
        "if os.path.exists(MODEL_FILENAME) and os.path.exists(VECTORIZER_FILENAME):\n",
        "    try:\n",
        "        loaded_model = joblib.load(MODEL_FILENAME)\n",
        "        loaded_vectorizer = joblib.load(VECTORIZER_FILENAME)\n",
        "        print(\"Multi-label model and vectorizer loaded for prediction.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading multi-label model/vectorizer: {e}\")\n",
        "else:\n",
        "    print(\"Multi-label model or vectorizer file not found. Prediction will not work.\")\n",
        "\n",
        "\n",
        "def classify_multilabel_and_highlight(comment):\n",
        "    if loaded_model is None or loaded_vectorizer is None:\n",
        "        return \"Model not loaded. Cannot classify.\", None, \"\"\n",
        "\n",
        "    if not comment or not isinstance(comment, str) or comment.isspace():\n",
        "         return \"Please enter some text.\", None, \"\"\n",
        "\n",
        "    cleaned_comment_for_model = preprocess_text(comment) # For TF-IDF and prediction\n",
        "    comment_tfidf = loaded_vectorizer.transform([cleaned_comment_for_model])\n",
        "\n",
        "    probabilities = loaded_model.predict_proba(comment_tfidf)[0]\n",
        "\n",
        "    results_text = \"Predicted Probabilities:\\n\"\n",
        "    any_label_toxic_predicted = False\n",
        "    prob_threshold_for_highlight = 0.3 # Lower threshold for triggering highlighting\n",
        "    prob_threshold_for_labeling = 0.5 # Threshold for saying a label is \"present\"\n",
        "\n",
        "    for i, label in enumerate(TOXIC_LABELS):\n",
        "        prob = probabilities[i]\n",
        "        results_text += f\"  - {label}: {prob:.4f}\\n\"\n",
        "        if prob > prob_threshold_for_highlight:\n",
        "            any_label_toxic_predicted = True\n",
        "\n",
        "    highlighted_output = []\n",
        "    # Tokenize while trying to keep punctuation as separate tokens for highlighting original words\n",
        "    original_words = re.findall(r\"[\\w']+|[^\\s\\w]\", comment)\n",
        "\n",
        "\n",
        "    if any_label_toxic_predicted:\n",
        "        for word_token in original_words:\n",
        "            # For matching, lemmatize and lower the word without its surrounding punctuation\n",
        "            processed_word_for_match = lemmatizer.lemmatize(word_token.lower().strip(string.punctuation))\n",
        "            if processed_word_for_match in GENERIC_TOXIC_KEYWORDS_SET and processed_word_for_match:\n",
        "                highlighted_output.append((word_token, \"Toxic\"))\n",
        "            else:\n",
        "                highlighted_output.append((word_token, None))\n",
        "    else:\n",
        "         highlighted_output = [(word_token, None) for word_token in original_words]\n",
        "\n",
        "    if not highlighted_output and comment and not comment.isspace(): # Ensure output if comment exists\n",
        "         highlighted_output = [(word_token, None) for word_token in original_words]\n",
        "\n",
        "    binary_predictions = (probabilities > prob_threshold_for_labeling).astype(int)\n",
        "    predicted_labels_str = \", \".join([TOXIC_LABELS[i] for i, pred in enumerate(binary_predictions) if pred == 1])\n",
        "    if not predicted_labels_str:\n",
        "        predicted_labels_str = \"None (below threshold)\"\n",
        "    summary_text = f\"Predicted Toxic Labels (Threshold > {prob_threshold_for_labeling}): {predicted_labels_str}\"\n",
        "\n",
        "    return results_text, highlighted_output, summary_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rKcy3DyE1mO",
        "outputId": "26f03a29-fb39-4c7c-daab-633157bb8b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-label model and vectorizer loaded for prediction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Build and Launch Gradio Web Interface (Multi-Label)\n",
        "\n",
        "if loaded_model and loaded_vectorizer:\n",
        "    print(\"Setting up Gradio interface for Multi-Label Classification...\")\n",
        "    iface = gr.Interface(\n",
        "        fn=classify_multilabel_and_highlight,\n",
        "        inputs=gr.Textbox(lines=5, label=\"Enter Comment Text\", placeholder=\"Type your comment here...\"),\n",
        "        outputs=[\n",
        "            gr.Textbox(label=\"Predicted Probabilities per Toxicity Type\"),\n",
        "            gr.HighlightedText(\n",
        "                label=\"Comment Analysis (Keywords highlighted if any toxicity type is probable)\",\n",
        "                color_map={\"Toxic\": \"#FF0000\"}\n",
        "            ),\n",
        "            gr.Textbox(label=\"Predicted Toxic Labels\")\n",
        "        ],\n",
        "        title=\"Multi-Label Toxic Comment Detection\",\n",
        "        description=(\n",
        "            \"Enter a comment to get probabilities for 6 types of toxicity: \"\n",
        "            f\"{', '.join(TOXIC_LABELS)}. \"\n",
        "            \"Keywords are highlighted if any toxicity type has a probability > 0.3. \"\n",
        "            \"Predicted labels are shown for probabilities > 0.5.\"\n",
        "        ),\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "\n",
        "    print(\"Launching Gradio interface...\")\n",
        "    iface.launch(share=True, debug=True)\n",
        "else:\n",
        "    print(\"Gradio interface cannot be launched as the multi-label model/vectorizer was not loaded/trained successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "8m6J8LGdE7_n",
        "outputId": "eaee8385-d237-4a5a-c3ef-790e7adce9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up Gradio interface for Multi-Label Classification...\n",
            "Launching Gradio interface...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://6a8e2b18d17932972c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://6a8e2b18d17932972c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Naaa41DFFFUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}